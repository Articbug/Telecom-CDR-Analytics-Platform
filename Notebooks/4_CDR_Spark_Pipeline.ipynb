{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs7zTwq9hpvYoQ9a8iYhdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Articbug/Telecom-CDR-Analytics-Platform/blob/main/Notebooks/4_CDR_Spark_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RflFuzpCO34I",
        "outputId": "61363b6c-33b7-4cca-d7d7-a1e294173fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-4.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cryptography>=44.0.1 (from snowflake-connector-python)\n",
            "  Downloading cryptography-46.0.5-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=24.0.0 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (2.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.4 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (2.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (26.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=3.7 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (3.11)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (2026.1.4)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (4.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (3.24.2)\n",
            "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (4.9.2)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (0.13.3)\n",
            "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
            "  Downloading boto3-1.42.54-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
            "  Downloading botocore-1.42.54-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=44.0.1->snowflake-connector-python) (2.0.0)\n",
            "INFO: pip is looking at multiple versions of pyopenssl to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyOpenSSL<26.0.0,>=24.0.0 (from snowflake-connector-python)\n",
            "  Downloading pyopenssl-25.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=44.0.1->snowflake-connector-python) (3.0)\n",
            "Downloading snowflake_connector_python-4.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.54-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.54-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-46.0.5-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, asn1crypto, jmespath, cryptography, botocore, s3transfer, pyOpenSSL, boto3, snowflake-connector-python\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.5 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asn1crypto-1.5.1 boto3-1.42.54 botocore-1.42.54 cryptography-46.0.5 jmespath-1.1.0 pyOpenSSL-25.3.0 s3transfer-0.16.0 snowflake-connector-python-4.3.0 sortedcontainers-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark snowflake-connector-python pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "\n",
        "# ‚îÄ‚îÄ Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('CDR_Analytics_Pipeline') \\\n",
        "    .config('spark.sql.shuffle.partitions', '4') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel('ERROR')\n",
        "print(f'‚úÖ Spark Version: {spark.version}')\n",
        "print(f'‚úÖ Spark Session started successfully')\n",
        "\n",
        "# ‚îÄ‚îÄ Read data from Snowflake into Pandas then convert to Spark\n",
        "print('\\nüì• Reading CDR data from Snowflake...')\n",
        "conn = snowflake.connector.connect(\n",
        "    account  = 'bopsoxz-lr52214',\n",
        "    user     = 'CHANDANSAHOO',\n",
        "    password = 'Chandansahoosnowflake5',\n",
        "    database = 'TELECOM_DWH',\n",
        "    schema   = 'STAGING',\n",
        "    warehouse= 'INGEST_WH'\n",
        ")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "cursor.execute('SELECT * FROM TELECOM_DWH.STAGING.STG_CDR')\n",
        "rows = cursor.fetchall()\n",
        "columns = [desc[0] for desc in cursor.description]\n",
        "pandas_df = pd.DataFrame(rows, columns=columns)\n",
        "conn.close()\n",
        "\n",
        "# ‚îÄ‚îÄ Convert to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(pandas_df)\n",
        "print(f'‚úÖ Loaded {spark_df.count():,} records into Spark DataFrame')\n",
        "print(f'‚úÖ Partitions: {spark_df.rdd.getNumPartitions()}')\n",
        "spark_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azvpyXMePUl5",
        "outputId": "6fd51960-56a4-49dc-f3ce-bc1260575c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark Version: 4.0.2\n",
            "‚úÖ Spark Session started successfully\n",
            "\n",
            "üì• Reading CDR data from Snowflake...\n",
            "‚úÖ Loaded 50,000 records into Spark DataFrame\n",
            "‚úÖ Partitions: 2\n",
            "root\n",
            " |-- CALL_ID: string (nullable = true)\n",
            " |-- CALLING_NUMBER: string (nullable = true)\n",
            " |-- CALLED_NUMBER: string (nullable = true)\n",
            " |-- CALL_START_TIME: timestamp (nullable = true)\n",
            " |-- CALL_END_TIME: timestamp (nullable = true)\n",
            " |-- DURATION_SECONDS: long (nullable = true)\n",
            " |-- CALL_TYPE: string (nullable = true)\n",
            " |-- CELL_ID: string (nullable = true)\n",
            " |-- TERMINATION_CD: string (nullable = true)\n",
            " |-- IS_ROAMING: boolean (nullable = true)\n",
            " |-- CHARGE_AMOUNT: decimal(38,18) (nullable = true)\n",
            " |-- DATA_VOLUME_MB: decimal(38,18) (nullable = true)\n",
            " |-- NETWORK_TYPE: string (nullable = true)\n",
            " |-- IS_FRAUD: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "print('üîÑ BATCH PROCESSING: Applying Spark Transformations...')\n",
        "print('=' * 55)\n",
        "\n",
        "# ‚îÄ‚îÄ Transformation 1: Add derived columns\n",
        "df_transformed = spark_df \\\n",
        "    .withColumn('CALL_HOUR',    hour('CALL_START_TIME')) \\\n",
        "    .withColumn('CALL_DATE',    to_date('CALL_START_TIME')) \\\n",
        "    .withColumn('CALL_MONTH',   month('CALL_START_TIME')) \\\n",
        "    .withColumn('CALL_YEAR',    year('CALL_START_TIME')) \\\n",
        "    .withColumn('IS_PEAK_HOUR', when(\n",
        "        (col('CALL_HOUR').between(8, 11)) |\n",
        "        (col('CALL_HOUR').between(17, 20)), True\n",
        "    ).otherwise(False)) \\\n",
        "    .withColumn('CHARGE_AMOUNT', col('CHARGE_AMOUNT').cast('double')) \\\n",
        "    .withColumn('DATA_VOLUME_MB', col('DATA_VOLUME_MB').cast('double'))\n",
        "\n",
        "print('‚úÖ Transformation 1: Derived columns added (hour, date, month, peak hour)')\n",
        "\n",
        "# ‚îÄ‚îÄ Transformation 2: Revenue segmentation\n",
        "df_transformed = df_transformed.withColumn(\n",
        "    'REVENUE_SEGMENT',\n",
        "    when(col('CHARGE_AMOUNT') >= 5.0,  'HIGH')\n",
        "    .when(col('CHARGE_AMOUNT') >= 1.0, 'MEDIUM')\n",
        "    .otherwise('LOW')\n",
        ")\n",
        "print('‚úÖ Transformation 2: Revenue segments assigned (HIGH/MEDIUM/LOW)')\n",
        "\n",
        "# ‚îÄ‚îÄ Transformation 3: Window function - Running total per subscriber\n",
        "window_spec = Window.partitionBy('CALLING_NUMBER').orderBy('CALL_START_TIME')\n",
        "df_transformed = df_transformed \\\n",
        "    .withColumn('RUNNING_REVENUE', sum('CHARGE_AMOUNT').over(window_spec)) \\\n",
        "    .withColumn('CALL_RANK',       rank().over(window_spec))\n",
        "print('‚úÖ Transformation 3: Running revenue + call rank per subscriber added')\n",
        "\n",
        "# ‚îÄ‚îÄ Transformation 4: Flag high value calls\n",
        "df_transformed = df_transformed.withColumn(\n",
        "    'IS_HIGH_VALUE',\n",
        "    when(col('CHARGE_AMOUNT') > col('CHARGE_AMOUNT').cast('double'), True)\n",
        "    .otherwise(False)\n",
        ")\n",
        "print('‚úÖ Transformation 4: High value call flags added')\n",
        "\n",
        "print(f'\\nüìä Transformed DataFrame: {df_transformed.count():,} records')\n",
        "print(f'   Columns: {len(df_transformed.columns)}')\n",
        "df_transformed.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Y8q-UWPxv8",
        "outputId": "8b8fdc6e-2bd4-40a5-afef-e2155743eb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ BATCH PROCESSING: Applying Spark Transformations...\n",
            "=======================================================\n",
            "‚úÖ Transformation 1: Derived columns added (hour, date, month, peak hour)\n",
            "‚úÖ Transformation 2: Revenue segments assigned (HIGH/MEDIUM/LOW)\n",
            "‚úÖ Transformation 3: Running revenue + call rank per subscriber added\n",
            "‚úÖ Transformation 4: High value call flags added\n",
            "\n",
            "üìä Transformed DataFrame: 50,000 records\n",
            "   Columns: 23\n",
            "+-----------+--------------+-------------+-------------------+-------------------+----------------+---------+----------+--------------+----------+-------------+--------------+------------+--------+---------+----------+----------+---------+------------+---------------+---------------+---------+-------------+\n",
            "|CALL_ID    |CALLING_NUMBER|CALLED_NUMBER|CALL_START_TIME    |CALL_END_TIME      |DURATION_SECONDS|CALL_TYPE|CELL_ID   |TERMINATION_CD|IS_ROAMING|CHARGE_AMOUNT|DATA_VOLUME_MB|NETWORK_TYPE|IS_FRAUD|CALL_HOUR|CALL_DATE |CALL_MONTH|CALL_YEAR|IS_PEAK_HOUR|REVENUE_SEGMENT|RUNNING_REVENUE|CALL_RANK|IS_HIGH_VALUE|\n",
            "+-----------+--------------+-------------+-------------------+-------------------+----------------+---------+----------+--------------+----------+-------------+--------------+------------+--------+---------+----------+----------+---------+------------+---------------+---------------+---------+-------------+\n",
            "|CDR00021435|916215334035  |918824508349 |2024-01-03 18:59:36|2024-01-03 19:01:06|90              |VOICE    |CELL_00003|NORMAL        |false     |0.75         |0.0           |4G          |false   |18       |2024-01-03|1         |2024     |true        |LOW            |0.75           |1        |false        |\n",
            "|CDR00015089|916215334035  |916280836292 |2024-01-09 13:47:34|2024-01-09 13:47:34|0               |SMS      |CELL_00002|DROPPED       |false     |0.1          |0.0           |4G          |false   |13       |2024-01-09|1         |2024     |false       |LOW            |0.85           |2        |false        |\n",
            "|CDR00025902|916215334035  |916540289135 |2024-01-14 20:05:16|2024-01-14 20:09:13|237             |VIDEO    |CELL_00004|NORMAL        |true      |7.9          |118.5         |4G          |false   |20       |2024-01-14|1         |2024     |true        |HIGH           |8.75           |3        |false        |\n",
            "|CDR00036312|916215334035  |916545859307 |2024-01-16 15:04:10|2024-01-16 15:10:42|392             |VOICE    |CELL_00003|NORMAL        |false     |3.2667       |0.0           |4G          |false   |15       |2024-01-16|1         |2024     |false       |MEDIUM         |12.0167        |4        |false        |\n",
            "|CDR00027680|916215334035  |916353455870 |2024-01-20 21:21:38|2024-01-20 21:21:52|14              |VOICE    |CELL_00004|DROPPED       |false     |0.1167       |0.0           |4G          |false   |21       |2024-01-20|1         |2024     |false       |LOW            |12.1334        |5        |false        |\n",
            "+-----------+--------------+-------------+-------------------+-------------------+----------------+---------+----------+--------------+----------+-------------+--------------+------------+--------+---------+----------+----------+---------+------------+---------------+---------------+---------+-------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count_distinct\n",
        "\n",
        "print('üìä SPARK ANALYTICS: Running Aggregations...')\n",
        "print('=' * 55)\n",
        "\n",
        "# ‚îÄ‚îÄ Analysis 1: Revenue by Call Type\n",
        "print('\\n1Ô∏è‚É£  Revenue by Call Type:')\n",
        "df_transformed.groupBy('CALL_TYPE') \\\n",
        "    .agg(\n",
        "        count('*').alias('total_calls'),\n",
        "        round(sum('CHARGE_AMOUNT'), 2).alias('total_revenue'),\n",
        "        round(avg('DURATION_SECONDS'), 1).alias('avg_duration')\n",
        "    ) \\\n",
        "    .orderBy(desc('total_revenue')) \\\n",
        "    .show()\n",
        "\n",
        "# ‚îÄ‚îÄ Analysis 2: Peak vs Off-Peak Traffic\n",
        "print('2Ô∏è‚É£  Peak vs Off-Peak Traffic:')\n",
        "df_transformed.groupBy('IS_PEAK_HOUR') \\\n",
        "    .agg(\n",
        "        count('*').alias('total_calls'),\n",
        "        round(sum('CHARGE_AMOUNT'), 2).alias('total_revenue'),\n",
        "        round(avg('CHARGE_AMOUNT'), 4).alias('avg_charge')\n",
        "    ) \\\n",
        "    .orderBy(desc('total_calls')) \\\n",
        "    .show()\n",
        "\n",
        "# ‚îÄ‚îÄ Analysis 3: Network Type Performance\n",
        "print('3Ô∏è‚É£  Network Type Performance:')\n",
        "df_transformed.groupBy('NETWORK_TYPE') \\\n",
        "    .agg(\n",
        "        count('*').alias('total_calls'),\n",
        "        round(sum('CHARGE_AMOUNT'), 2).alias('total_revenue'),\n",
        "        count(when(col('TERMINATION_CD') == 'DROPPED', 1)).alias('dropped_calls')\n",
        "    ) \\\n",
        "    .orderBy(desc('total_calls')) \\\n",
        "    .show()\n",
        "\n",
        "# ‚îÄ‚îÄ Analysis 4: Monthly Revenue Trend\n",
        "print('4Ô∏è‚É£  Monthly Revenue Trend:')\n",
        "df_transformed.groupBy('CALL_MONTH') \\\n",
        "    .agg(\n",
        "        count('*').alias('total_calls'),\n",
        "        round(sum('CHARGE_AMOUNT'), 2).alias('total_revenue'),\n",
        "        count_distinct('CALLING_NUMBER').alias('unique_customers')\n",
        "    ) \\\n",
        "    .orderBy('CALL_MONTH') \\\n",
        "    .show(12)\n",
        "\n",
        "# ‚îÄ‚îÄ Analysis 5: Fraud Summary\n",
        "print('5Ô∏è‚É£  Fraud Summary:')\n",
        "df_transformed.groupBy('IS_FRAUD') \\\n",
        "    .agg(\n",
        "        count('*').alias('total_calls'),\n",
        "        round(sum('CHARGE_AMOUNT'), 2).alias('total_revenue')\n",
        "    ) \\\n",
        "    .show()\n",
        "\n",
        "print('‚úÖ All Spark Analytics completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4qUwfnzQEM0",
        "outputId": "75c7416a-9b3c-45b9-d8fb-fdaac455f498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä SPARK ANALYTICS: Running Aggregations...\n",
            "=======================================================\n",
            "\n",
            "1Ô∏è‚É£  Revenue by Call Type:\n",
            "+---------+-----------+-------------+------------+\n",
            "|CALL_TYPE|total_calls|total_revenue|avg_duration|\n",
            "+---------+-----------+-------------+------------+\n",
            "|    VOICE|      27405|     41652.23|       179.1|\n",
            "|    VIDEO|       2533|     12726.12|       294.3|\n",
            "|     DATA|       7605|       3758.0|       609.6|\n",
            "|      SMS|      12457|       1269.2|         0.0|\n",
            "+---------+-----------+-------------+------------+\n",
            "\n",
            "2Ô∏è‚É£  Peak vs Off-Peak Traffic:\n",
            "+------------+-----------+-------------+----------+\n",
            "|IS_PEAK_HOUR|total_calls|total_revenue|avg_charge|\n",
            "+------------+-----------+-------------+----------+\n",
            "|        true|      27147|     32468.01|     1.196|\n",
            "|       false|      22853|     26937.53|    1.1787|\n",
            "+------------+-----------+-------------+----------+\n",
            "\n",
            "3Ô∏è‚É£  Network Type Performance:\n",
            "+------------+-----------+-------------+-------------+\n",
            "|NETWORK_TYPE|total_calls|total_revenue|dropped_calls|\n",
            "+------------+-----------+-------------+-------------+\n",
            "|          4G|      27586|     32943.76|         3930|\n",
            "|          5G|      12465|     14753.32|         1769|\n",
            "|          3G|       7540|      8933.47|         1105|\n",
            "|          2G|       2409|       2775.0|          339|\n",
            "+------------+-----------+-------------+-------------+\n",
            "\n",
            "4Ô∏è‚É£  Monthly Revenue Trend:\n",
            "+----------+-----------+-------------+----------------+\n",
            "|CALL_MONTH|total_calls|total_revenue|unique_customers|\n",
            "+----------+-----------+-------------+----------------+\n",
            "|         1|       4265|      5052.67|             499|\n",
            "|         2|       4064|      4817.58|             500|\n",
            "|         3|       4212|       5001.3|             499|\n",
            "|         4|       4042|      4736.15|             500|\n",
            "|         5|       4260|      5094.61|             500|\n",
            "|         6|       4115|      4892.69|             500|\n",
            "|         7|       4189|      4979.05|             500|\n",
            "|         8|       4128|      5094.25|             500|\n",
            "|         9|       4053|      4794.34|             500|\n",
            "|        10|       4229|      5072.27|             500|\n",
            "|        11|       4226|      4989.19|             500|\n",
            "|        12|       4217|      4881.43|             500|\n",
            "+----------+-----------+-------------+----------------+\n",
            "\n",
            "5Ô∏è‚É£  Fraud Summary:\n",
            "+--------+-----------+-------------+\n",
            "|IS_FRAUD|total_calls|total_revenue|\n",
            "+--------+-----------+-------------+\n",
            "|   false|      49750|     59084.01|\n",
            "|    true|        250|       321.54|\n",
            "+--------+-----------+-------------+\n",
            "\n",
            "‚úÖ All Spark Analytics completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import builtins\n",
        "builtins_round = builtins.round"
      ],
      "metadata": {
        "id": "E5zy1IK_Qp6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, when, sum as spark_sum, count\n",
        "\n",
        "print('üåä SPARK STREAMING SIMULATION')\n",
        "print('=' * 55)\n",
        "print('Simulating real-time CDR records arriving every second...\\n')\n",
        "\n",
        "# ‚îÄ‚îÄ Define schema\n",
        "schema = StructType([\n",
        "    StructField('CALL_ID',          StringType(),  True),\n",
        "    StructField('CALLING_NUMBER',   StringType(),  True),\n",
        "    StructField('CALLED_NUMBER',    StringType(),  True),\n",
        "    StructField('CALL_TYPE',        StringType(),  True),\n",
        "    StructField('DURATION_SECONDS', IntegerType(), True),\n",
        "    StructField('CHARGE_AMOUNT',    DoubleType(),  True),\n",
        "    StructField('NETWORK_TYPE',     StringType(),  True),\n",
        "    StructField('IS_FRAUD',         BooleanType(), True),\n",
        "    StructField('TIMESTAMP',        StringType(),  True),\n",
        "])\n",
        "\n",
        "PHONES     = ['919199123456', '918877654321', '917766543210',\n",
        "              '916655432109', '919988776655']\n",
        "CALL_TYPES = ['VOICE', 'SMS', 'DATA', 'VIDEO']\n",
        "NETWORKS   = ['2G', '3G', '4G', '5G']\n",
        "\n",
        "total_processed = 0\n",
        "total_revenue   = 0.0\n",
        "fraud_detected  = 0\n",
        "\n",
        "print(f'{\"Batch\":<8}{\"Records\":<10}{\"Revenue\":<12}{\"Fraud\":<8}{\"Time\"}')\n",
        "print('-' * 55)\n",
        "\n",
        "for batch_num in range(1, 11):\n",
        "    batch_records = []\n",
        "    for i in range(10):\n",
        "        call_type = random.choice(CALL_TYPES)\n",
        "        duration  = random.randint(0, 300)\n",
        "        charge    = builtins_round(random.uniform(0.1, 10.0), 2)\n",
        "        is_fraud  = random.random() < 0.05\n",
        "\n",
        "        batch_records.append((\n",
        "            f'STREAM_{batch_num:03d}_{i:03d}',\n",
        "            random.choice(PHONES),\n",
        "            random.choice(PHONES),\n",
        "            call_type,\n",
        "            duration,\n",
        "            charge,\n",
        "            random.choice(NETWORKS),\n",
        "            is_fraud,\n",
        "            datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        ))\n",
        "\n",
        "    batch_df = spark.createDataFrame(batch_records, schema)\n",
        "\n",
        "    batch_df = batch_df \\\n",
        "        .withColumn('REVENUE_FLAG',\n",
        "            when(col('CHARGE_AMOUNT') > 5.0, 'HIGH')\n",
        "            .when(col('CHARGE_AMOUNT') > 2.0, 'MEDIUM')\n",
        "            .otherwise('LOW')) \\\n",
        "        .withColumn('FRAUD_ALERT',\n",
        "            when(col('IS_FRAUD') == True, 'FRAUD DETECTED')\n",
        "            .otherwise('NORMAL'))\n",
        "\n",
        "    batch_stats = batch_df.agg(\n",
        "        count('*').alias('cnt'),\n",
        "        spark_sum('CHARGE_AMOUNT').alias('rev'),\n",
        "        spark_sum(when(col('IS_FRAUD') == True, 1).otherwise(0)).alias('fraud')\n",
        "    ).collect()[0]\n",
        "\n",
        "    total_processed += batch_stats['cnt']\n",
        "    total_revenue   += float(batch_stats['rev'])\n",
        "    fraud_detected  += int(batch_stats['fraud'])\n",
        "\n",
        "    rev = builtins_round(float(batch_stats['rev']), 2)\n",
        "    print(f'{batch_num:<8}{batch_stats[\"cnt\"]:<10}‚Çπ{rev:<11}{batch_stats[\"fraud\"]:<8}{datetime.now().strftime(\"%H:%M:%S\")}')\n",
        "\n",
        "    frauds = batch_df.filter(col('IS_FRAUD') == True).select(\n",
        "        'CALL_ID', 'CALLING_NUMBER', 'CHARGE_AMOUNT'\n",
        "    ).collect()\n",
        "    for f in frauds:\n",
        "        print(f'   ‚ö†Ô∏è  ALERT: {f[\"CALL_ID\"]} | {f[\"CALLING_NUMBER\"]} | ‚Çπ{f[\"CHARGE_AMOUNT\"]}')\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "print('-' * 55)\n",
        "print(f'\\nüìä STREAMING SUMMARY:')\n",
        "print(f'   Total Records Processed: {total_processed}')\n",
        "print(f'   Total Revenue:           ‚Çπ{builtins_round(total_revenue, 2)}')\n",
        "print(f'   Fraud Alerts:            {fraud_detected}')\n",
        "print(f'\\n‚úÖ Spark Streaming Simulation Complete!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G1hXNJTRB9q",
        "outputId": "b0c4dae0-fda2-4343-c2d5-b1b621e364b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåä SPARK STREAMING SIMULATION\n",
            "=======================================================\n",
            "Simulating real-time CDR records arriving every second...\n",
            "\n",
            "Batch   Records   Revenue     Fraud   Time\n",
            "-------------------------------------------------------\n",
            "1       10        ‚Çπ64.78      2       12:13:04\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_001_001 | 919199123456 | ‚Çπ8.42\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_001_009 | 919988776655 | ‚Çπ8.59\n",
            "2       10        ‚Çπ50.14      1       12:13:07\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_002_001 | 918877654321 | ‚Çπ5.29\n",
            "3       10        ‚Çπ55.13      0       12:13:09\n",
            "4       10        ‚Çπ51.96      1       12:13:11\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_004_008 | 916655432109 | ‚Çπ4.48\n",
            "5       10        ‚Çπ55.82      2       12:13:13\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_005_003 | 919988776655 | ‚Çπ2.49\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_005_009 | 919988776655 | ‚Çπ9.15\n",
            "6       10        ‚Çπ45.7       1       12:13:15\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_006_001 | 918877654321 | ‚Çπ5.52\n",
            "7       10        ‚Çπ50.32      0       12:13:17\n",
            "8       10        ‚Çπ53.36      1       12:13:20\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_008_007 | 916655432109 | ‚Çπ1.51\n",
            "9       10        ‚Çπ51.34      1       12:13:22\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_009_005 | 916655432109 | ‚Çπ5.01\n",
            "10      10        ‚Çπ47.26      1       12:13:24\n",
            "   ‚ö†Ô∏è  ALERT: STREAM_010_003 | 919988776655 | ‚Çπ2.41\n",
            "-------------------------------------------------------\n",
            "\n",
            "üìä STREAMING SUMMARY:\n",
            "   Total Records Processed: 100\n",
            "   Total Revenue:           ‚Çπ525.81\n",
            "   Fraud Alerts:            10\n",
            "\n",
            "‚úÖ Spark Streaming Simulation Complete!\n"
          ]
        }
      ]
    }
  ]
}